import os
import base64
from PIL import Image
import pytesseract
import requests
import argparse
from tqdm import tqdm

def process_image_with_tesseract(image_path):
    """Perform OCR using Tesseract"""
    try:
        image = Image.open(image_path)
        # Perform OCR using Tesseract with the specified language model
        text = pytesseract.image_to_string(image, 'fas') # Language set to Farsi ('fas')
        return text
    except Exception as e:
        return f"Error processing with Tesseract: {str(e)}"

def encode_image(image_path):
    """Encode image to a base64 string"""
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def process_image_with_ollama(image_path, tesseract_text, use_two_step=False):
    """
    Process an image using the native Ollama API.
    
    Args:
        image_path (str): The path to the image file.
        tesseract_text (str): The OCR text generated by Tesseract.
        use_two_step (bool): If True, performs a second API call to refine the result.
    
    Returns:
        tuple: A tuple containing (initial_response, final_response).
               If one-step is used, final_response will be None.
    """
    encoded_image = encode_image(image_path)
    
    ollama_endpoint = 'http://localhost:11434/api/chat'
    model_name = "gemma3:27b-it-q8_0" # Ensure this multimodal model is available

    initial_messages = [
        {
            "role": "user",
            "content": "OCR text in this image. dont translate. dont add any extra text.",
            "images": [encoded_image]
        }
    ]

    payload = {
        "model": model_name,
        "messages": initial_messages,
        "stream": False
    }

    try:
        response = requests.post(ollama_endpoint, json=payload)
        response.raise_for_status()
        result = response.json()
        initial_response_content = result['message']['content']

        # If one-step OCR is selected, return the initial result and None for the final
        if not use_two_step:
            return initial_response_content, None

        # --- Two-Step Refinement Process ---
        follow_up_messages = [
            *initial_messages,
            { "role": "assistant", "content": initial_response_content },
            { "role": "user", "content": f"Combine your OCR results with this Tesseract output and refine your response: {tesseract_text}" }
        ]
        
        follow_up_payload = { "model": model_name, "messages": follow_up_messages, "stream": False }
        follow_up_response = requests.post(ollama_endpoint, json=follow_up_payload)
        follow_up_response.raise_for_status()
        follow_up_result = follow_up_response.json()
        final_response_content = follow_up_result['message']['content']
        
        return initial_response_content, final_response_content

    except requests.exceptions.RequestException as e:
        error_message = f"Error processing image with Ollama: {str(e)}"
        return error_message, None
    except KeyError as e:
        error_message = f"Error parsing Ollama response: {str(e)}. Response: {response.text}"
        return error_message, None

def main(folder_path, output_folder, use_two_step):
    """Main function to process all images in a folder."""
    os.makedirs(output_folder, exist_ok=True)

    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

    all_tesseract_results = []
    all_ollama_results = []

    for image_file in tqdm(image_files, desc="Processing images"):
        image_path = os.path.join(folder_path, image_file)
        
        tesseract_text = process_image_with_tesseract(image_path)
        
        # Get both initial and final (if applicable) results from Ollama
        ollama_initial, ollama_final = process_image_with_ollama(image_path, tesseract_text, use_two_step)
        
        # Determine the final text to save in the main output and aggregate results
        final_ollama_text = ollama_final if use_two_step and ollama_final else ollama_initial
        
        base_name = os.path.splitext(image_file)[0]
        tesseract_output_file = os.path.join(output_folder, f"{base_name}_tesseract.txt")
        ollama_output_file = os.path.join(output_folder, f"{base_name}_ollama.txt")

        # Save Tesseract result
        with open(tesseract_output_file, 'w', encoding='utf-8') as tes_file:
            tes_file.write(tesseract_text)
        
        # If in two-step mode, save the intermediate AI response
        if use_two_step:
            intermediate_output_file = os.path.join(output_folder, f"{base_name}_ollama_intermediate.txt")
            with open(intermediate_output_file, 'w', encoding='utf-8') as inter_file:
                inter_file.write(ollama_initial or "")

        # Save the final AI response
        with open(ollama_output_file, 'w', encoding='utf-8') as ollama_file:
            ollama_file.write(final_ollama_text or "")
        
        # Collect results for aggregation
        all_tesseract_results.append(f"--- {image_file} ---\n{tesseract_text}\n")
        all_ollama_results.append(f"--- {image_file} ---\n{final_ollama_text or ''}\n")

    # Save aggregated results
    with open(os.path.join(output_folder, "all_tesseract_results.txt"), 'w', encoding='utf-8') as agg_file:
        agg_file.write('\n'.join(all_tesseract_results))

    with open(os.path.join(output_folder, "all_ollama_results.txt"), 'w', encoding='utf-8') as agg_file:
        agg_file.write('\n'.join(all_ollama_results))

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Process a folder of images with Tesseract and Ollama for OCR.')
    parser.add_argument('-i', '--input', required=True, help='Path to the folder containing images.')
    parser.add_argument('-o', '--output', required=True, help='Path to the folder where text files will be saved.')
    parser.add_argument('--two-step', action='store_true', help='Enable two-step AI refinement using Tesseract output. This will also save the intermediate AI response.')

    args = parser.parse_args()

    main(args.input, args.output, args.two_step)